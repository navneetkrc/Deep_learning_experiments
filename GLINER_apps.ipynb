{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyreIz+y3yGQXZLwF2xmmk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8dab5a04bdf640fa97897014f9aa95c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_81fb85680bd74fbd8f72d3e0431a515a",
              "IPY_MODEL_49d2f8cac051414b91e81699654fc4ff",
              "IPY_MODEL_2a903d573a6a49cda6062cab07f7de52"
            ],
            "layout": "IPY_MODEL_4bd940234832487fa3a8ad7490462c04"
          }
        },
        "81fb85680bd74fbd8f72d3e0431a515a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98bde25866d54b9eb4543c076b67a140",
            "placeholder": "​",
            "style": "IPY_MODEL_e173e6e808bd42cba90be5df7729eca7",
            "value": "Fetching 4 files: 100%"
          }
        },
        "49d2f8cac051414b91e81699654fc4ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0459f15004c24e6cbf438b88f0c34992",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d72e190bf40f447688bfbbcbdc930920",
            "value": 4
          }
        },
        "2a903d573a6a49cda6062cab07f7de52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9079b8052ac642b7ad863cee45a20077",
            "placeholder": "​",
            "style": "IPY_MODEL_0d00cebd627b4708bbda61984c01c712",
            "value": " 4/4 [00:00&lt;00:00, 210.78it/s]"
          }
        },
        "4bd940234832487fa3a8ad7490462c04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98bde25866d54b9eb4543c076b67a140": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e173e6e808bd42cba90be5df7729eca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0459f15004c24e6cbf438b88f0c34992": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d72e190bf40f447688bfbbcbdc930920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9079b8052ac642b7ad863cee45a20077": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d00cebd627b4708bbda61984c01c712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cbe125d4db0045a7b246fd03978a4b5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b70c46b3d65d423f996721a5297b533f",
              "IPY_MODEL_7a3ed674abe34a6c93332494dc32394e",
              "IPY_MODEL_2502755cfb124dc78622033c3566045c"
            ],
            "layout": "IPY_MODEL_a71fed124dde409f9fdf0035fdb6b3f5"
          }
        },
        "b70c46b3d65d423f996721a5297b533f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ec990abef774b6ab35682aace5612bd",
            "placeholder": "​",
            "style": "IPY_MODEL_266540bd25e840c0ad35b3f58b9fc665",
            "value": "Fetching 4 files: 100%"
          }
        },
        "7a3ed674abe34a6c93332494dc32394e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a0164302a8a46e2aaeafb87e7cf28d5",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70060481ea084606b2c56b41422ba641",
            "value": 4
          }
        },
        "2502755cfb124dc78622033c3566045c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02d2f49e10524552b06a1dd8f303554c",
            "placeholder": "​",
            "style": "IPY_MODEL_54e91b8a8c1e40989382a527735407b7",
            "value": " 4/4 [00:00&lt;00:00, 194.75it/s]"
          }
        },
        "a71fed124dde409f9fdf0035fdb6b3f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ec990abef774b6ab35682aace5612bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "266540bd25e840c0ad35b3f58b9fc665": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a0164302a8a46e2aaeafb87e7cf28d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70060481ea084606b2c56b41422ba641": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "02d2f49e10524552b06a1dd8f303554c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54e91b8a8c1e40989382a527735407b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navneetkrc/Deep_learning_experiments/blob/master/GLINER_apps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "# !pip install gliner pycountry scipy==1.12 gradio==4.31.5 spaces"
      ],
      "metadata": {
        "id": "gTQ9q4bqFlZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8dab5a04bdf640fa97897014f9aa95c6",
            "81fb85680bd74fbd8f72d3e0431a515a",
            "49d2f8cac051414b91e81699654fc4ff",
            "2a903d573a6a49cda6062cab07f7de52",
            "4bd940234832487fa3a8ad7490462c04",
            "98bde25866d54b9eb4543c076b67a140",
            "e173e6e808bd42cba90be5df7729eca7",
            "0459f15004c24e6cbf438b88f0c34992",
            "d72e190bf40f447688bfbbcbdc930920",
            "9079b8052ac642b7ad863cee45a20077",
            "0d00cebd627b4708bbda61984c01c712"
          ]
        },
        "id": "lpDKV9hhEuC9",
        "outputId": "ff89b246-715d-46d2-b548-c80e077b4e85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cache directory: None\n",
            "Initializing models...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8dab5a04bdf640fa97897014f9aa95c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-26 21:14:12.552231 :: get_model :: 0:00:08.268311\n",
            "2025-02-26 21:14:13.743901 :: predict_entities :: 0:00:09.459982\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "IMPORTANT: You are using gradio version 4.31.5, however version 4.44.1 is available, please upgrade.\n",
            "--------\n",
            "Running on public URL: https://c2c94330f8da742cdd.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c2c94330f8da742cdd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/type_adapter.py\", line 271, in _init_core_attrs\n",
            "    self.core_schema = _getattr_no_parents(self._type, '__pydantic_core_schema__')\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/type_adapter.py\", line 55, in _getattr_no_parents\n",
            "    raise AttributeError(attribute)\n",
            "AttributeError: __pydantic_core_schema__\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/applications.py\", line 1054, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/applications.py\", line 112, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 713, in __call__\n",
            "    await self.simple_response(scope, receive, send, request_headers=headers)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 729, in simple_response\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 715, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 735, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 288, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 76, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 73, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 291, in app\n",
            "    solved_result = await solve_dependencies(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/dependencies/utils.py\", line 666, in solve_dependencies\n",
            "    ) = await request_body_to_args(  # body_params checked above\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/dependencies/utils.py\", line 891, in request_body_to_args\n",
            "    fields_to_extract = get_cached_model_fields(first_field.type_)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/_compat.py\", line 659, in get_cached_model_fields\n",
            "    return get_model_fields(model)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/_compat.py\", line 285, in get_model_fields\n",
            "    return [\n",
            "           ^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/_compat.py\", line 286, in <listcomp>\n",
            "    ModelField(field_info=field_info, name=name)\n",
            "  File \"<string>\", line 6, in __init__\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/_compat.py\", line 111, in __post_init__\n",
            "    self._type_adapter: TypeAdapter[Any] = TypeAdapter(\n",
            "                                           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/type_adapter.py\", line 228, in __init__\n",
            "    self._init_core_attrs(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/type_adapter.py\", line 290, in _init_core_attrs\n",
            "    core_schema = schema_generator.generate_schema(self._type)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 610, in generate_schema\n",
            "    schema = self._generate_schema_inner(obj)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 863, in _generate_schema_inner\n",
            "    return self._annotated_schema(obj)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 1977, in _annotated_schema\n",
            "    schema = self._apply_annotations(source_type, annotations)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 2056, in _apply_annotations\n",
            "    schema = get_inner_schema(source_type)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_schema_generation_shared.py\", line 84, in __call__\n",
            "    schema = self._handler(source_type)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 2131, in new_handler\n",
            "    schema = metadata_get_schema(source, get_inner_schema)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 2127, in <lambda>\n",
            "    lambda source, handler: handler(source)\n",
            "                            ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_schema_generation_shared.py\", line 84, in __call__\n",
            "    schema = self._handler(source_type)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 2037, in inner_handler\n",
            "    schema = self._generate_schema_inner(obj)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 884, in _generate_schema_inner\n",
            "    return self.match_type(obj)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 986, in match_type\n",
            "    return self._match_generic_type(obj, origin)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 1014, in _match_generic_type\n",
            "    return self._union_schema(obj)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 1325, in _union_schema\n",
            "    choices.append(self.generate_schema(arg))\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 610, in generate_schema\n",
            "    schema = self._generate_schema_inner(obj)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 884, in _generate_schema_inner\n",
            "    return self.match_type(obj)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 995, in match_type\n",
            "    return self._unknown_type_schema(obj)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 513, in _unknown_type_schema\n",
            "    raise PydanticSchemaGenerationError(\n",
            "pydantic.errors.PydanticSchemaGenerationError: Unable to generate pydantic-core schema for <class 'starlette.requests.Request'>. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\n",
            "\n",
            "If you got this error by calling handler(<some type>) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(<some type>)` since we do not call `__get_pydantic_core_schema__` on `<some type>` otherwise to avoid infinite recursion.\n",
            "\n",
            "For further information visit https://errors.pydantic.dev/2.10/u/schema-for-unknown-type\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://c2c94330f8da742cdd.gradio.live\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import json\n",
        "import gradio as gr\n",
        "import pycountry\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from typing import Dict, Union\n",
        "from gliner import GLiNER\n",
        "\n",
        "# Model configuration\n",
        "_MODEL = {}\n",
        "_CACHE_DIR = os.environ.get(\"CACHE_DIR\", None)\n",
        "THRESHOLD = 0.3\n",
        "LABELS = [\"product\", \"product type\", \"price\", \"memory\", \"feature\"]\n",
        "QUERY = \"What is the difference between the Samsung Galaxy S23 and S23 Ultra?, What does 'Dynamic AMOLED 2X' mean in a Samsung display?, How do I use the S Pen on my Samsung Galaxy Note or Ultra phone?, What is 'One UI' on Samsung phones?, What is 'Samsung Knox' and why is it important?, What is the difference between Samsung QLED and Neo QLED TVs?, What is 'SmartThings' in Samsung appliances?, What is 'Bespoke' in Samsung refrigerators?, What does 'EcoBubble' mean in Samsung washing machines?, What is the Samsung Galaxy Watch and what are its features?, What are Samsung's different memory and storage devices?, What is the difference between a Samsung soundbar and a home theater system?\"\n",
        "MODELS = [\"urchade/gliner_base\", \"urchade/gliner_medium-v2.1\", \"urchade/gliner_multi-v2.1\", \"urchade/gliner_large-v2.1\"]\n",
        "\n",
        "print(f\"Cache directory: {_CACHE_DIR}\")\n",
        "\n",
        "def get_model(model_name: str = None):\n",
        "    start = datetime.now()\n",
        "\n",
        "    if model_name is None:\n",
        "        model_name = \"urchade/gliner_base\"\n",
        "\n",
        "    global _MODEL\n",
        "\n",
        "    if _MODEL.get(model_name) is None:\n",
        "        _MODEL[model_name] = GLiNER.from_pretrained(model_name, cache_dir=_CACHE_DIR)\n",
        "\n",
        "    if torch.cuda.is_available() and not next(_MODEL[model_name].parameters()).device.type.startswith(\"cuda\"):\n",
        "        _MODEL[model_name] = _MODEL[model_name].to(\"cuda\")\n",
        "\n",
        "    print(f\"{datetime.now()} :: get_model :: {datetime.now() - start}\")\n",
        "\n",
        "    return _MODEL[model_name]\n",
        "\n",
        "def get_country(country_name: str):\n",
        "    try:\n",
        "        return pycountry.countries.search_fuzzy(country_name)\n",
        "    except LookupError:\n",
        "        return None\n",
        "\n",
        "# Removed @spaces.GPU decorator since it's not needed in Colab\n",
        "def predict_entities(model_name: str, query: str, labels: Union[str, list], threshold: float = 0.3, nested_ner: bool = False):\n",
        "    start = datetime.now()\n",
        "    model = get_model(model_name)\n",
        "\n",
        "    if isinstance(labels, str):\n",
        "        labels = [i.strip() for i in labels.split(\",\")]\n",
        "\n",
        "    entities = model.predict_entities(query, labels, threshold=threshold, flat_ner=not nested_ner)\n",
        "\n",
        "    print(f\"{datetime.now()} :: predict_entities :: {datetime.now() - start}\")\n",
        "\n",
        "    return entities\n",
        "\n",
        "def parse_query(query: str, labels: Union[str, list], threshold: float = 0.3, nested_ner: bool = False, model_name: str = None) -> Dict[str, Union[str, list]]:\n",
        "    entities = []\n",
        "    _entities = predict_entities(model_name=model_name, query=query, labels=labels, threshold=threshold, nested_ner=nested_ner)\n",
        "\n",
        "    for entity in _entities:\n",
        "        if entity[\"label\"] == \"country\":\n",
        "            country = get_country(entity[\"text\"])\n",
        "            if country:\n",
        "                entity[\"normalized\"] = [dict(c) for c in country]\n",
        "                entities.append(entity)\n",
        "        else:\n",
        "            entities.append(entity)\n",
        "\n",
        "    payload = {\"query\": query, \"entities\": entities}\n",
        "    print(f\"{datetime.now()} :: parse_query :: {json.dumps(payload)}\\n\")\n",
        "\n",
        "    return payload\n",
        "\n",
        "def annotate_query(query: str, labels: Union[str, list], threshold: float = 0.3, nested_ner: bool = False, model_name: str = None) -> Dict[str, Union[str, list]]:\n",
        "    payload = parse_query(query, labels, threshold, nested_ner, model_name)\n",
        "\n",
        "    return {\n",
        "        \"text\": query,\n",
        "        \"entities\": [\n",
        "            {\n",
        "                \"entity\": entity[\"label\"],\n",
        "                \"word\": entity[\"text\"],\n",
        "                \"start\": entity[\"start\"],\n",
        "                \"end\": entity[\"end\"],\n",
        "                \"score\": entity[\"score\"],\n",
        "            }\n",
        "            for entity in payload[\"entities\"]\n",
        "        ],\n",
        "    }\n",
        "\n",
        "# Function to create and launch the Gradio interface\n",
        "def create_gliner_interface():\n",
        "    # Initialize model here\n",
        "    print(\"Initializing models...\")\n",
        "    # Only initialize the base model by default to save time\n",
        "    predict_entities(\"urchade/gliner_base\", QUERY, LABELS, threshold=THRESHOLD)\n",
        "\n",
        "    with gr.Blocks(title=\"GLiNER-query-parser\") as demo:\n",
        "        gr.Markdown(\n",
        "        \"\"\"\n",
        "        # GLiNER-based Query Parser (a zero-shot NER model)\n",
        "        This demonstrates the GLiNER model's ability to predict entities in a given text query. Given a set of entities to track, the model can then identify instances of these entities in the query. The parsed entities are then displayed in the output. A special case is the \"country\" entity, which is normalized to the ISO 3166-1 alpha-2 code using the pycountry library. This GLiNER mode is licensed under the Apache 2.0 license.\n",
        "        ## Links\n",
        "        * Model: https://huggingface.co/urchade/gliner_medium-v2.1, https://huggingface.co/urchade/gliner_base\n",
        "        * All GLiNER models: https://huggingface.co/models?library=gliner\n",
        "        * Paper: https://arxiv.org/abs/2311.08526\n",
        "        * Repository: https://github.com/urchade/GLiNER\n",
        "        \"\"\"\n",
        "        )\n",
        "\n",
        "        query = gr.Textbox(\n",
        "            value=QUERY, label=\"query\", placeholder=\"Enter your query here\"\n",
        "        )\n",
        "        with gr.Row() as row:\n",
        "            model_name = gr.Radio(\n",
        "                choices=MODELS,\n",
        "                value=\"urchade/gliner_base\",\n",
        "                label=\"Model\",\n",
        "            )\n",
        "            entities = gr.Textbox(\n",
        "                value=\", \".join(LABELS),\n",
        "                label=\"entities\",\n",
        "                placeholder=\"Enter the entities to detect here (comma separated)\",\n",
        "                scale=2,\n",
        "            )\n",
        "            threshold = gr.Slider(\n",
        "                0,\n",
        "                1,\n",
        "                value=THRESHOLD,\n",
        "                step=0.01,\n",
        "                label=\"Threshold\",\n",
        "                info=\"Lower threshold may extract more false-positive entities from the query.\",\n",
        "                scale=1,\n",
        "            )\n",
        "            is_nested = gr.Checkbox(\n",
        "                value=False,\n",
        "                label=\"Nested NER\",\n",
        "                info=\"Setting to True extracts nested entities\",\n",
        "                scale=0,\n",
        "            )\n",
        "\n",
        "        output = gr.HighlightedText(label=\"Annotated entities\")\n",
        "        submit_btn = gr.Button(\"Submit\")\n",
        "\n",
        "        # Submitting\n",
        "        query.submit(\n",
        "            fn=annotate_query, inputs=[query, entities, threshold, is_nested, model_name], outputs=output\n",
        "        )\n",
        "        entities.submit(\n",
        "            fn=annotate_query, inputs=[query, entities, threshold, is_nested, model_name], outputs=output\n",
        "        )\n",
        "        threshold.release(\n",
        "            fn=annotate_query, inputs=[query, entities, threshold, is_nested, model_name], outputs=output\n",
        "        )\n",
        "        submit_btn.click(\n",
        "            fn=annotate_query, inputs=[query, entities, threshold, is_nested, model_name], outputs=output\n",
        "        )\n",
        "        is_nested.change(\n",
        "            fn=annotate_query, inputs=[query, entities, threshold, is_nested, model_name], outputs=output\n",
        "        )\n",
        "        model_name.change(\n",
        "            fn=annotate_query, inputs=[query, entities, threshold, is_nested, model_name], outputs=output\n",
        "        )\n",
        "\n",
        "        # Configure for Colab environment\n",
        "        # Using share=True to generate a public URL\n",
        "        demo.queue(default_concurrency_limit=5)\n",
        "        demo.launch(debug=True, share=True)\n",
        "\n",
        "    return demo\n",
        "\n",
        "# This will be called from a Colab cell\n",
        "if __name__ == \"__main__\":\n",
        "    create_gliner_interface()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QNlplMITFjni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GLiNER Query Parser - Google Colab Implementation\n",
        "\n",
        "# Cell 1: Install Required Libraries\n",
        "# !pip install gliner pycountry scipy==1.12 gradio==4.31.5 spaces\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FnIjEcQ7Eu-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Import Libraries and Define Functions\n",
        "import os\n",
        "import json\n",
        "import gradio as gr\n",
        "import pycountry\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from typing import Dict, Union\n",
        "from gliner import GLiNER\n"
      ],
      "metadata": {
        "id": "tt-pTWqaFKAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Model Configuration\n",
        "_MODEL = {}\n",
        "_CACHE_DIR = os.environ.get(\"CACHE_DIR\", None)\n",
        "THRESHOLD = 0.3\n",
        "LABELS = [\"country\", \"year\", \"statistical indicator\", \"geographic region\"]\n",
        "QUERY = \"gdp, co2 emissions, and mortality rate of the philippines vs. south asia in 2024\"\n",
        "MODELS = [\"urchade/gliner_base\", \"urchade/gliner_medium-v2.1\", \"urchade/gliner_multi-v2.1\", \"urchade/gliner_large-v2.1\"]\n",
        "\n",
        "print(f\"Cache directory: {_CACHE_DIR}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2iiYuNeFJ9X",
        "outputId": "94250700-bc33-4e7d-e892-a307bc534168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cache directory: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Define Helper Functions\n",
        "def get_model(model_name: str = None):\n",
        "    start = datetime.now()\n",
        "\n",
        "    if model_name is None:\n",
        "        model_name = \"urchade/gliner_base\"\n",
        "\n",
        "    global _MODEL\n",
        "\n",
        "    if _MODEL.get(model_name) is None:\n",
        "        _MODEL[model_name] = GLiNER.from_pretrained(model_name, cache_dir=_CACHE_DIR)\n",
        "\n",
        "    if torch.cuda.is_available() and not next(_MODEL[model_name].parameters()).device.type.startswith(\"cuda\"):\n",
        "        _MODEL[model_name] = _MODEL[model_name].to(\"cuda\")\n",
        "\n",
        "    print(f\"{datetime.now()} :: get_model :: {datetime.now() - start}\")\n",
        "\n",
        "    return _MODEL[model_name]\n",
        "\n",
        "def get_country(country_name: str):\n",
        "    try:\n",
        "        return pycountry.countries.search_fuzzy(country_name)\n",
        "    except LookupError:\n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "id": "-VUuGsbRFAR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Define Prediction Functions\n",
        "def predict_entities(model_name: str, query: str, labels: Union[str, list], threshold: float = 0.3, nested_ner: bool = False):\n",
        "    start = datetime.now()\n",
        "    model = get_model(model_name)\n",
        "\n",
        "    if isinstance(labels, str):\n",
        "        labels = [i.strip() for i in labels.split(\",\")]\n",
        "\n",
        "    entities = model.predict_entities(query, labels, threshold=threshold, flat_ner=not nested_ner)\n",
        "\n",
        "    print(f\"{datetime.now()} :: predict_entities :: {datetime.now() - start}\")\n",
        "\n",
        "    return entities\n",
        "\n",
        "def parse_query(query: str, labels: Union[str, list], threshold: float = 0.3, nested_ner: bool = False, model_name: str = None) -> Dict[str, Union[str, list]]:\n",
        "    entities = []\n",
        "    _entities = predict_entities(model_name=model_name, query=query, labels=labels, threshold=threshold, nested_ner=nested_ner)\n",
        "\n",
        "    for entity in _entities:\n",
        "        if entity[\"label\"] == \"country\":\n",
        "            country = get_country(entity[\"text\"])\n",
        "            if country:\n",
        "                entity[\"normalized\"] = [dict(c) for c in country]\n",
        "                entities.append(entity)\n",
        "        else:\n",
        "            entities.append(entity)\n",
        "\n",
        "    payload = {\"query\": query, \"entities\": entities}\n",
        "    print(f\"{datetime.now()} :: parse_query :: {json.dumps(payload)}\\n\")\n",
        "\n",
        "    return payload\n",
        "\n",
        "def annotate_query(query: str, labels: Union[str, list], threshold: float = 0.3, nested_ner: bool = False, model_name: str = None) -> Dict[str, Union[str, list]]:\n",
        "    payload = parse_query(query, labels, threshold, nested_ner, model_name)\n",
        "\n",
        "    return {\n",
        "        \"text\": query,\n",
        "        \"entities\": [\n",
        "            {\n",
        "                \"entity\": entity[\"label\"],\n",
        "                \"word\": entity[\"text\"],\n",
        "                \"start\": entity[\"start\"],\n",
        "                \"end\": entity[\"end\"],\n",
        "                \"score\": entity[\"score\"],\n",
        "            }\n",
        "            for entity in payload[\"entities\"]\n",
        "        ],\n",
        "    }\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wjhvO1gSFAU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Initialize Model and Create Gradio Interface\n",
        "# Initialize only base model to save time\n",
        "print(\"Initializing base model...\")\n",
        "predict_entities(\"urchade/gliner_base\", QUERY, LABELS, threshold=THRESHOLD)\n",
        "\n",
        "# Create the Gradio interface\n",
        "def create_interface():\n",
        "    with gr.Blocks(title=\"GLiNER-query-parser\") as demo:\n",
        "        gr.Markdown(\n",
        "        \"\"\"\n",
        "        # GLiNER-based Query Parser (a zero-shot NER model)\n",
        "        This demonstrates the GLiNER model's ability to predict entities in a given text query.\n",
        "        The model identifies instances of specified entities in the query, and the parsed entities are displayed in the output.\n",
        "        \"\"\"\n",
        "        )\n",
        "\n",
        "        query = gr.Textbox(\n",
        "            value=QUERY, label=\"Query\", placeholder=\"Enter your query here\"\n",
        "        )\n",
        "        with gr.Row() as row:\n",
        "            model_name = gr.Radio(\n",
        "                choices=MODELS,\n",
        "                value=\"urchade/gliner_base\",\n",
        "                label=\"Model\",\n",
        "            )\n",
        "            entities = gr.Textbox(\n",
        "                value=\", \".join(LABELS),\n",
        "                label=\"Entities\",\n",
        "                placeholder=\"Enter the entities to detect here (comma separated)\",\n",
        "                scale=2,\n",
        "            )\n",
        "            threshold = gr.Slider(\n",
        "                0,\n",
        "                1,\n",
        "                value=THRESHOLD,\n",
        "                step=0.01,\n",
        "                label=\"Threshold\",\n",
        "                info=\"Lower threshold may extract more false-positive entities from the query.\",\n",
        "                scale=1,\n",
        "            )\n",
        "            is_nested = gr.Checkbox(\n",
        "                value=False,\n",
        "                label=\"Nested NER\",\n",
        "                info=\"Setting to True extracts nested entities\",\n",
        "                scale=0,\n",
        "            )\n",
        "\n",
        "        output = gr.HighlightedText(label=\"Annotated entities\")\n",
        "        submit_btn = gr.Button(\"Submit\")\n",
        "\n",
        "        # Submitting\n",
        "        query.submit(\n",
        "            fn=annotate_query, inputs=[query, entities, threshold, is_nested, model_name], outputs=output\n",
        "        )\n",
        "        entities.submit(\n",
        "            fn=annotate_query, inputs=[query, entities, threshold, is_nested, model_name], outputs=output\n",
        "        )\n",
        "        threshold.release(\n",
        "            fn=annotate_query, inputs=[query, entities, threshold, is_nested, model_name], outputs=output\n",
        "        )\n",
        "        submit_btn.click(\n",
        "            fn=annotate_query, inputs=[query, entities, threshold, is_nested, model_name], outputs=output\n",
        "        )\n",
        "        is_nested.change(\n",
        "            fn=annotate_query, inputs=[query, entities, threshold, is_nested, model_name], outputs=output\n",
        "        )\n",
        "        model_name.change(\n",
        "            fn=annotate_query, inputs=[query, entities, threshold, is_nested, model_name], outputs=output\n",
        "        )\n",
        "\n",
        "        # Enable public URL sharing for Colab\n",
        "        demo.queue(default_concurrency_limit=5)\n",
        "        return demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "cbe125d4db0045a7b246fd03978a4b5f",
            "b70c46b3d65d423f996721a5297b533f",
            "7a3ed674abe34a6c93332494dc32394e",
            "2502755cfb124dc78622033c3566045c",
            "a71fed124dde409f9fdf0035fdb6b3f5",
            "2ec990abef774b6ab35682aace5612bd",
            "266540bd25e840c0ad35b3f58b9fc665",
            "6a0164302a8a46e2aaeafb87e7cf28d5",
            "70060481ea084606b2c56b41422ba641",
            "02d2f49e10524552b06a1dd8f303554c",
            "54e91b8a8c1e40989382a527735407b7"
          ]
        },
        "id": "VuUvuHsuFAXy",
        "outputId": "0b6a45f7-6630-43eb-c0b4-f1b829c6e9e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing base model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cbe125d4db0045a7b246fd03978a4b5f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-26 21:16:17.141558 :: get_model :: 0:00:07.807576\n",
            "2025-02-26 21:16:17.745978 :: predict_entities :: 0:00:08.412004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Launch the App\n",
        "# This will output a public URL that you can access from anywhere\n",
        "interface = create_interface()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dKt3SB8oFAaY",
        "outputId": "7e9e70fb-8f44-4c64-bdff-8f55e1fb417c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "IMPORTANT: You are using gradio version 4.31.5, however version 4.44.1 is available, please upgrade.\n",
            "--------\n",
            "Running on public URL: https://67f370a7541491a9c1.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://67f370a7541491a9c1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/type_adapter.py\", line 271, in _init_core_attrs\n",
            "    self.core_schema = _getattr_no_parents(self._type, '__pydantic_core_schema__')\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/type_adapter.py\", line 55, in _getattr_no_parents\n",
            "    raise AttributeError(attribute)\n",
            "AttributeError: __pydantic_core_schema__\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/applications.py\", line 1054, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/applications.py\", line 112, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 713, in __call__\n",
            "    await self.simple_response(scope, receive, send, request_headers=headers)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 729, in simple_response\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 715, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 735, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 288, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 76, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 73, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 291, in app\n",
            "    solved_result = await solve_dependencies(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/dependencies/utils.py\", line 666, in solve_dependencies\n",
            "    ) = await request_body_to_args(  # body_params checked above\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/dependencies/utils.py\", line 891, in request_body_to_args\n",
            "    fields_to_extract = get_cached_model_fields(first_field.type_)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/_compat.py\", line 659, in get_cached_model_fields\n",
            "    return get_model_fields(model)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/_compat.py\", line 285, in get_model_fields\n",
            "    return [\n",
            "           ^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/_compat.py\", line 286, in <listcomp>\n",
            "    ModelField(field_info=field_info, name=name)\n",
            "  File \"<string>\", line 6, in __init__\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/_compat.py\", line 111, in __post_init__\n",
            "    self._type_adapter: TypeAdapter[Any] = TypeAdapter(\n",
            "                                           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/type_adapter.py\", line 228, in __init__\n",
            "    self._init_core_attrs(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/type_adapter.py\", line 290, in _init_core_attrs\n",
            "    core_schema = schema_generator.generate_schema(self._type)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 610, in generate_schema\n",
            "    schema = self._generate_schema_inner(obj)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 863, in _generate_schema_inner\n",
            "    return self._annotated_schema(obj)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 1977, in _annotated_schema\n",
            "    schema = self._apply_annotations(source_type, annotations)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 2056, in _apply_annotations\n",
            "    schema = get_inner_schema(source_type)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_schema_generation_shared.py\", line 84, in __call__\n",
            "    schema = self._handler(source_type)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 2131, in new_handler\n",
            "    schema = metadata_get_schema(source, get_inner_schema)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 2127, in <lambda>\n",
            "    lambda source, handler: handler(source)\n",
            "                            ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_schema_generation_shared.py\", line 84, in __call__\n",
            "    schema = self._handler(source_type)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 2037, in inner_handler\n",
            "    schema = self._generate_schema_inner(obj)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 884, in _generate_schema_inner\n",
            "    return self.match_type(obj)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 986, in match_type\n",
            "    return self._match_generic_type(obj, origin)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 1014, in _match_generic_type\n",
            "    return self._union_schema(obj)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 1325, in _union_schema\n",
            "    choices.append(self.generate_schema(arg))\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 610, in generate_schema\n",
            "    schema = self._generate_schema_inner(obj)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 884, in _generate_schema_inner\n",
            "    return self.match_type(obj)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 995, in match_type\n",
            "    return self._unknown_type_schema(obj)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py\", line 513, in _unknown_type_schema\n",
            "    raise PydanticSchemaGenerationError(\n",
            "pydantic.errors.PydanticSchemaGenerationError: Unable to generate pydantic-core schema for <class 'starlette.requests.Request'>. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\n",
            "\n",
            "If you got this error by calling handler(<some type>) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(<some type>)` since we do not call `__get_pydantic_core_schema__` on `<some type>` otherwise to avoid infinite recursion.\n",
            "\n",
            "For further information visit https://errors.pydantic.dev/2.10/u/schema-for-unknown-type\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://67f370a7541491a9c1.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Test API Usage (Optional)\n",
        "# You can uncomment this to test the API\n",
        "'''\n",
        "from gradio_client import Client\n",
        "\n",
        "# Use the public URL that was generated above\n",
        "client = Client(\"YOUR_PUBLIC_URL_HERE\")\n",
        "result = client.predict(\n",
        "    query=\"gdp, m3, and child mortality of india and southeast asia 2024\",\n",
        "    labels=\"country, year, statistical indicator, region\",\n",
        "    threshold=0.3,\n",
        "    nested_ner=False,\n",
        "    api_name=\"/parse_query\"\n",
        ")\n",
        "print(result)\n",
        "'''"
      ],
      "metadata": {
        "id": "dsOmY9bjFAqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hwz_RkIWFrMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##streamlit app"
      ],
      "metadata": {
        "id": "tHg_r82QFq6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Install required packages\n",
        "# !pip install gliner pycountry scipy==1.12 streamlit torch\n",
        "\n",
        "# import os\n",
        "# import json\n",
        "# import streamlit as st\n",
        "# import pycountry\n",
        "# import torch\n",
        "# from datetime import datetime\n",
        "# from typing import Dict, Union\n",
        "# from gliner import GLiNER\n",
        "\n",
        "# # Model configuration\n",
        "# _MODEL = {}\n",
        "# _CACHE_DIR = os.environ.get(\"CACHE_DIR\", None)\n",
        "# THRESHOLD = 0.3\n",
        "# LABELS = [\"country\", \"year\", \"statistical indicator\", \"geographic region\"]\n",
        "# QUERY = \"gdp, co2 emissions, and mortality rate of the philippines vs. south asia in 2024\"\n",
        "# MODELS = [\"urchade/gliner_base\", \"urchade/gliner_medium-v2.1\", \"urchade/gliner_multi-v2.1\", \"urchade/gliner_large-v2.1\"]\n",
        "\n",
        "# # Helper functions\n",
        "# def get_model(model_name: str = None):\n",
        "#     if model_name is None:\n",
        "#         model_name = \"urchade/gliner_base\"\n",
        "\n",
        "#     global _MODEL\n",
        "\n",
        "#     if _MODEL.get(model_name) is None:\n",
        "#         _MODEL[model_name] = GLiNER.from_pretrained(model_name, cache_dir=_CACHE_DIR)\n",
        "\n",
        "#     if torch.cuda.is_available() and not next(_MODEL[model_name].parameters()).device.type.startswith(\"cuda\"):\n",
        "#         _MODEL[model_name] = _MODEL[model_name].to(\"cuda\")\n",
        "\n",
        "#     return _MODEL[model_name]\n",
        "\n",
        "# def get_country(country_name: str):\n",
        "#     try:\n",
        "#         return pycountry.countries.search_fuzzy(country_name)\n",
        "#     except LookupError:\n",
        "#         return None\n",
        "\n",
        "# def predict_entities(model_name: str, query: str, labels: Union[str, list], threshold: float = 0.3, nested_ner: bool = False):\n",
        "#     model = get_model(model_name)\n",
        "\n",
        "#     if isinstance(labels, str):\n",
        "#         labels = [i.strip() for i in labels.split(\",\")]\n",
        "\n",
        "#     entities = model.predict_entities(query, labels, threshold=threshold, flat_ner=not nested_ner)\n",
        "\n",
        "#     return entities\n",
        "\n",
        "# def parse_query(query: str, labels: Union[str, list], threshold: float = 0.3, nested_ner: bool = False, model_name: str = None) -> Dict[str, Union[str, list]]:\n",
        "#     entities = []\n",
        "#     _entities = predict_entities(model_name=model_name, query=query, labels=labels, threshold=threshold, nested_ner=nested_ner)\n",
        "\n",
        "#     for entity in _entities:\n",
        "#         if entity[\"label\"] == \"country\":\n",
        "#             country = get_country(entity[\"text\"])\n",
        "#             if country:\n",
        "#                 entity[\"normalized\"] = [dict(c) for c in country]\n",
        "#                 entities.append(entity)\n",
        "#         else:\n",
        "#             entities.append(entity)\n",
        "\n",
        "#     return {\"query\": query, \"entities\": entities}\n",
        "\n",
        "# # Initialize only base model\n",
        "# @st.cache_resource\n",
        "# def initialize_model():\n",
        "#     return get_model(\"urchade/gliner_base\")\n",
        "\n",
        "# # Pre-initialize the model\n",
        "# init_model = initialize_model()\n",
        "\n",
        "# # Create the Streamlit interface\n",
        "# st.title(\"GLiNER-based Query Parser\")\n",
        "# st.markdown(\"\"\"\n",
        "# This app demonstrates the GLiNER model's ability to predict entities in a given text query.\n",
        "# Given a set of entities to track, the model can identify instances of these entities in the query.\n",
        "# \"\"\")\n",
        "\n",
        "# # User inputs\n",
        "# query = st.text_area(\"Query\", value=QUERY, placeholder=\"Enter your query here\")\n",
        "# model_name = st.radio(\"Model\", options=MODELS, index=0)\n",
        "# entities = st.text_input(\"Entities to detect (comma separated)\", value=\", \".join(LABELS))\n",
        "# threshold = st.slider(\"Threshold\", min_value=0.0, max_value=1.0, value=THRESHOLD, step=0.01,\n",
        "#                      help=\"Lower threshold may extract more false-positive entities from the query.\")\n",
        "# is_nested = st.checkbox(\"Nested NER\", value=False, help=\"Setting to True extracts nested entities\")\n",
        "\n",
        "# # Process when the form is submitted\n",
        "# if st.button(\"Submit\"):\n",
        "#     with st.spinner(\"Processing...\"):\n",
        "#         # Process the query\n",
        "#         result = parse_query(query, entities, threshold, is_nested, model_name)\n",
        "\n",
        "#         # Display the annotated text\n",
        "#         st.subheader(\"Detected Entities\")\n",
        "\n",
        "#         # Create highlighted text display\n",
        "#         text = result[\"query\"]\n",
        "#         annotations = []\n",
        "\n",
        "#         for entity in result[\"entities\"]:\n",
        "#             annotations.append({\n",
        "#                 \"start\": entity[\"start\"],\n",
        "#                 \"end\": entity[\"end\"],\n",
        "#                 \"label\": entity[\"label\"],\n",
        "#                 \"score\": round(entity[\"score\"], 3)\n",
        "#             })\n",
        "\n",
        "#         # Sort annotations by start position\n",
        "#         annotations.sort(key=lambda x: x[\"start\"])\n",
        "\n",
        "#         # Display original text with annotations\n",
        "#         processed_text = \"\"\n",
        "#         last_end = 0\n",
        "\n",
        "#         for annotation in annotations:\n",
        "#             # Add text before the entity\n",
        "#             processed_text += text[last_end:annotation[\"start\"]]\n",
        "#             # Add highlighted entity\n",
        "#             entity_text = text[annotation[\"start\"]:annotation[\"end\"]]\n",
        "#             processed_text += f\"**[{entity_text}]({annotation['label']}, {annotation['score']:.3f})**\"\n",
        "#             last_end = annotation[\"end\"]\n",
        "\n",
        "#         # Add any remaining text\n",
        "#         processed_text += text[last_end:]\n",
        "\n",
        "#         st.markdown(processed_text)\n",
        "\n",
        "#         # Display raw JSON output for more details\n",
        "#         with st.expander(\"Raw JSON Output\"):\n",
        "#             st.json(result)\n",
        "\n",
        "# # Save this file as \"streamlit_app.py\"\n",
        "# # Run it in Colab with: !streamlit run streamlit_app.py -- --server.port=8501 --server.enableCORS=False --server.enableXsrfProtection=False\n",
        "\n",
        "# # To get a public URL, you can use ngrok\n",
        "# # !pip install pyngrok\n",
        "# # from pyngrok import ngrok\n",
        "# # public_url = ngrok.connect(port=8501)\n",
        "# # print(f\"Public URL: {public_url}\")"
      ],
      "metadata": {
        "id": "ZPzZaScFFsLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WaY40uWDH1vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile streamlit_app.py\n",
        "# Install required packages\n",
        "import os\n",
        "import json\n",
        "import streamlit as st\n",
        "import pycountry\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from typing import Dict, Union\n",
        "from gliner import GLiNER\n",
        "\n",
        "# Model configuration\n",
        "_MODEL = {}\n",
        "_CACHE_DIR = os.environ.get(\"CACHE_DIR\", None)\n",
        "THRESHOLD = 0.3\n",
        "LABELS = [\"country\", \"year\", \"statistical indicator\", \"geographic region\"]\n",
        "QUERY = \"gdp, co2 emissions, and mortality rate of the philippines vs. south asia in 2024\"\n",
        "MODELS = [\"urchade/gliner_base\", \"urchade/gliner_medium-v2.1\", \"urchade/gliner_multi-v2.1\", \"urchade/gliner_large-v2.1\"]\n",
        "\n",
        "# Helper functions\n",
        "def get_model(model_name: str = None):\n",
        "    if model_name is None:\n",
        "        model_name = \"urchade/gliner_base\"\n",
        "\n",
        "    global _MODEL\n",
        "\n",
        "    if _MODEL.get(model_name) is None:\n",
        "        _MODEL[model_name] = GLiNER.from_pretrained(model_name, cache_dir=_CACHE_DIR)\n",
        "\n",
        "    if torch.cuda.is_available() and not next(_MODEL[model_name].parameters()).device.type.startswith(\"cuda\"):\n",
        "        _MODEL[model_name] = _MODEL[model_name].to(\"cuda\")\n",
        "\n",
        "    return _MODEL[model_name]\n",
        "\n",
        "def get_country(country_name: str):\n",
        "    try:\n",
        "        return pycountry.countries.search_fuzzy(country_name)\n",
        "    except LookupError:\n",
        "        return None\n",
        "\n",
        "def predict_entities(model_name: str, query: str, labels: Union[str, list], threshold: float = 0.3, nested_ner: bool = False):\n",
        "    model = get_model(model_name)\n",
        "\n",
        "    if isinstance(labels, str):\n",
        "        labels = [i.strip() for i in labels.split(\",\")]\n",
        "\n",
        "    entities = model.predict_entities(query, labels, threshold=threshold, flat_ner=not nested_ner)\n",
        "\n",
        "    return entities\n",
        "\n",
        "def parse_query(query: str, labels: Union[str, list], threshold: float = 0.3, nested_ner: bool = False, model_name: str = None) -> Dict[str, Union[str, list]]:\n",
        "    entities = []\n",
        "    _entities = predict_entities(model_name=model_name, query=query, labels=labels, threshold=threshold, nested_ner=nested_ner)\n",
        "\n",
        "    for entity in _entities:\n",
        "        if entity[\"label\"] == \"country\":\n",
        "            country = get_country(entity[\"text\"])\n",
        "            if country:\n",
        "                entity[\"normalized\"] = [dict(c) for c in country]\n",
        "                entities.append(entity)\n",
        "        else:\n",
        "            entities.append(entity)\n",
        "\n",
        "    return {\"query\": query, \"entities\": entities}\n",
        "\n",
        "# Initialize only base model\n",
        "@st.cache_resource\n",
        "def initialize_model():\n",
        "    return get_model(\"urchade/gliner_base\")\n",
        "\n",
        "# Pre-initialize the model\n",
        "init_model = initialize_model()\n",
        "\n",
        "# Create the Streamlit interface\n",
        "st.title(\"GLiNER-based Query Parser\")\n",
        "st.markdown(\"\"\"\n",
        "This app demonstrates the GLiNER model's ability to predict entities in a given text query.\n",
        "Given a set of entities to track, the model can identify instances of these entities in the query.\n",
        "\"\"\")\n",
        "\n",
        "# User inputs\n",
        "query = st.text_area(\"Query\", value=QUERY, placeholder=\"Enter your query here\")\n",
        "model_name = st.radio(\"Model\", options=MODELS, index=0)\n",
        "entities = st.text_input(\"Entities to detect (comma separated)\", value=\", \".join(LABELS))\n",
        "threshold = st.slider(\"Threshold\", min_value=0.0, max_value=1.0, value=THRESHOLD, step=0.01,\n",
        "                     help=\"Lower threshold may extract more false-positive entities from the query.\")\n",
        "is_nested = st.checkbox(\"Nested NER\", value=False, help=\"Setting to True extracts nested entities\")\n",
        "\n",
        "# Process when the form is submitted\n",
        "if st.button(\"Submit\"):\n",
        "    with st.spinner(\"Processing...\"):\n",
        "        # Process the query\n",
        "        result = parse_query(query, entities, threshold, is_nested, model_name)\n",
        "\n",
        "        # Display the annotated text\n",
        "        st.subheader(\"Detected Entities\")\n",
        "\n",
        "        # Create highlighted text display\n",
        "        text = result[\"query\"]\n",
        "        annotations = []\n",
        "\n",
        "        for entity in result[\"entities\"]:\n",
        "            annotations.append({\n",
        "                \"start\": entity[\"start\"],\n",
        "                \"end\": entity[\"end\"],\n",
        "                \"label\": entity[\"label\"],\n",
        "                \"score\": round(entity[\"score\"], 3)\n",
        "            })\n",
        "\n",
        "        # Sort annotations by start position\n",
        "        annotations.sort(key=lambda x: x[\"start\"])\n",
        "\n",
        "        # Display original text with annotations\n",
        "        processed_text = \"\"\n",
        "        last_end = 0\n",
        "\n",
        "        for annotation in annotations:\n",
        "            # Add text before the entity\n",
        "            processed_text += text[last_end:annotation[\"start\"]]\n",
        "            # Add highlighted entity\n",
        "            entity_text = text[annotation[\"start\"]:annotation[\"end\"]]\n",
        "            processed_text += f\"**[{entity_text}]({annotation['label']}, {annotation['score']:.3f})**\"\n",
        "            last_end = annotation[\"end\"]\n",
        "\n",
        "        # Add any remaining text\n",
        "        processed_text += text[last_end:]\n",
        "\n",
        "        st.markdown(processed_text)\n",
        "\n",
        "        # Display raw JSON output for more details\n",
        "        with st.expander(\"Raw JSON Output\"):\n",
        "            st.json(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVsweRTTH1yk",
        "outputId": "98d4bd0a-dacc-4f59-8b98-e9c20320066b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install gliner pycountry scipy==1.12 streamlit torch pyngrok"
      ],
      "metadata": {
        "id": "M5-EavDTH11s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up ngrok authentication\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get the ngrok auth token from Colab secrets\n",
        "NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN2')\n",
        "\n",
        "# Configure ngrok with the auth token\n",
        "!ngrok authtoken $NGROK_AUTH_TOKEN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOW2__jGH14H",
        "outputId": "9abd7135-b114-4efb-d5f0-97e3b96e97a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Streamlit app in the background\n",
        "!nohup streamlit run streamlit_app.py --server.port=8509 --server.enableCORS=False --server.enableXsrfProtection=False &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uhm2_EcIIco",
        "outputId": "16eac803-a1d5-46e1-ef7a-5ef1f0e4412a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative: Use ngrok directly via command line\n",
        "!ngrok http 8509 --log=stdout > /dev/null &\n",
        "\n",
        "# Wait a moment for ngrok to start\n",
        "import time\n",
        "time.sleep(5)\n",
        "\n",
        "# Get the public URL from ngrok API\n",
        "import requests\n",
        "import json\n",
        "tunnels = json.loads(requests.get('http://localhost:4040/api/tunnels').text)\n",
        "public_url = tunnels['tunnels'][0]['public_url']\n",
        "print(f\"Public URL: {public_url}\")"
      ],
      "metadata": {
        "id": "Xvn0BYjvINYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zfjrmKkDIROv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}